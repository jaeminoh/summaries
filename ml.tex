\section{Machine Learning}

\subsection{Ainsworth, 2021 \cite{ainsworth2021}}

Plateau phonemenon may be occured during GD-based $l^2$-loss optimization.
Due to this, we don't know when to stop the iteration.

In this paper, the authors concentrated on the "activation pattern" of neurons
and verified that the plateau corresponds to the period when the activation pattern remains constant.
Specifically, 
the $V_t$ component of the loss decays to the staionary point exponentially 
during the plateau interval.
They proposed the method "active neuron least squares" (ANSL)
that finds the best activation pattern among candidates generated by non-arbitrary manipulation
and fits local least square linear lines to the data.
This optimization procedure is done without any gradient information.
It significantly reduces computation time compared to existing methods, such as Adam or GD.

\subsection{Lu, 2021 \cite{DeepONet}}
Universal approximation theorem is a theoretical justification of NN's performance in function approximation.
There is a similar theorem for not just a function, but an operator between two Banach spaces.
With this theorem, the authors proposed Deep Operator Network (DeepONet) to approximate an operator.

DeepONet consists of two subnetworks, branch net and trunk net.
Branch net eats discretized function with specified sensor $(x_1, \dots, x_m)$,
and trunk net eats $y$ the point at which the output function will be evaluated.
Specifically,
\[
    G^{NN}[u](y) = \sum_{i =1}^p b_i t_i
\]
where $(b_1, \dots, b_p) = B(u(x_1), \dots, u(x_m))$,
and $(t_i, \dots, t_p) = T(y)$.

It performs quite well when there is large, high-fidelity dataset.
One great advantage of this is, 
its prediction speed is very fast compared to exisiting simulation methods.
Once the network is trained, we can exploit the speed and get a lot of simulation results within a small time interval.
However, obtaining high-fidelity dataset is very expensive.
Instead of using high-fidelity data only,
we can use governing PDEs and low-fidelity data to lower the cost.

\subsection{Zhu, 2022 \cite{DeepONet-extrapolation}}

NN's ability of interpolation is well known.
However, in real world application,
one cannot avoid the situation of extrapolation.
The extrapolation problem of NN is easily identified,
by training it to approximate $\sin (\pi x)$ on $x\in[0,1]$.

In this paper, the extrapolation of Deep Operator Network is systemically studied.
As inputs of branch net, 
discretized functions from Gaussian Random Field with Gaussian kernel are usually used.
Here the extrapolation means, training DeepONet with GRF class of $l_{train}$,
and predicting the output with GRF class of $l_{test} \ne l_{train}$.
As $W_2$ distance of test function space and training function space increases,
the test error increases polynomially.
To obtain consistent result with extrapolation,
the authors suggests to fine-tune the pre-trained DeepONet in two-ways.
First, when governing PDEs are known, fine-tune the network with PINN loss
\[
    \frac{\lambda_1}{R}\sum_R \|\mathcal{F}[G^{NN}[v](\xi);v]\|^2 + \frac{\lambda_2}{B}\sum_B \|\mathcal{B}[G^{NN}[v](\xi);v]\|^2,
\]
where the first term is residual part, and the second term is boundary and initial condition.
Second, when sparse observations are given, fine-tune the network with those observations.
However, in the second case, over-fitting and catastrophic forgetting may be happen.
By fine-tuning together with training data points, forgetting can be avoided.
Here the loss is
\[
    \frac{1}{N_{train}}\sum_{i=1}^{N_{train}} \|G^{NN}[v](\xi_i) - G[v](\xi_i)\|^2 + \frac{\lambda}{N_{obs}}\sum_{j=1}^{N_{obs}}\|G^{NN}[v](\xi_j)-G[v](\xi_j)\|^2,
\]
where $i$ denotes index for training data, and $j$ denotes index for sparse observation.
