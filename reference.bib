@article{rosenbaum1983,
    author = {ROSENBAUM, PAUL R. and RUBIN, DONALD B.},
    title = "{The central role of the propensity score in observational studies for causal effects}",
    journal = {Biometrika},
    volume = {70},
    number = {1},
    pages = {41-55},
    year = {1983},
    month = {04},
    abstract = "{The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a two- dimensional plot.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/70.1.41},
    url = {https://doi.org/10.1093/biomet/70.1.41},
    eprint = {https://academic.oup.com/biomet/article-pdf/70/1/41/662954/70-1-41.pdf},
}

@article{rubin1974,
  title={Estimating causal effects of treatments in randomized and nonrandomized studies.},
  author={Donald B. Rubin},
  journal={Journal of Educational Psychology},
  year={1974},
  volume={66},
  pages={688-701}
}

@article{gpsboosting2015,
  title={A Boosting Algorithm for Estimating Generalized Propensity Scores with Continuous Treatments.},
  author={Zhu Y, Coffman DL, Ghosh D.},
  journal={Journal of Causal Inference},
  year={2015},
  volume={3(1)},
  pages={25-40},
  doi={10.1515/jci-2014-0022}
}

@article{angrist2018,
author = {Joshua D. Angrist and Òscar Jordà and Guido M. Kuersteiner},
title = {Semiparametric Estimates of Monetary Policy Effects: String Theory Revisited},
journal = {Journal of Business \& Economic Statistics},
volume = {36},
number = {3},
pages = {371-387},
year  = {2018},
publisher = {Taylor & Francis},
doi = {10.1080/07350015.2016.1204919},

URL = { 
        https://doi.org/10.1080/07350015.2016.1204919
    
},
eprint = { 
        https://doi.org/10.1080/07350015.2016.1204919
    
}
,
    abstract = { We develop flexible semiparametric time series methods for the estimation of the causal effect of monetary policy on macroeconomic aggregates. Our estimator captures the average causal response to discrete policy interventions in a macrodynamic setting, without the need for assumptions about the process generating macroeconomic outcomes. The proposed estimation strategy, based on propensity score weighting, easily accommodates asymmetric and nonlinear responses. Using this estimator, we show that monetary tightening has clear effects on the yield curve and on economic activity. Monetary accommodation, however, appears to generate less pronounced responses from both. Estimates for recent financial crisis years display a similarly dampened response to monetary accommodation. }
}

@article{holland1986,
author = { Paul W.   Holland },
title = {Statistics and Causal Inference},
journal = {Journal of the American Statistical Association},
volume = {81},
number = {396},
pages = {945-960},
year  = {1986},
publisher = {Taylor \& Francis},
doi = {10.1080/01621459.1986.10478354},

URL = { 
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1986.10478354
    
},
eprint = { 
        https://www.tandfonline.com/doi/pdf/10.1080/01621459.1986.10478354
    
}

}

@article {hirano2004,
	title = {The Propensity Score with Continuous Treatments},
	year = {2004},
	author = {Guido Imbens and Keisuke Hirano}
}

@article{mbb1989,
author = {Hans R. Kunsch},
title = {{The Jackknife and the Bootstrap for General Stationary Observations}},
volume = {17},
journal = {The Annals of Statistics},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {1217 -- 1241},
abstract = {We extend the jackknife and the bootstrap method of estimating standard errors to the case where the observations form a general stationary sequence. We do not attempt a reduction to i.i.d. values. The jackknife calculates the sample variance of replicates of the statistic obtained by omitting each block of $l$ consecutive data once. In the case of the arithmetic mean this is shown to be equivalent to a weighted covariance estimate of the spectral density of the observations at zero. Under appropriate conditions consistency is obtained if $l = l(n) \rightarrow \infty$ and $l(n)/n \rightarrow 0$. General statistics are approximated by an arithmetic mean. In regular cases this approximation determines the asymptotic behavior. Bootstrap replicates are constructed by selecting blocks of length $l$ randomly with replacement among the blocks of observations. The procedures are illustrated by using the sunspot numbers and some simulated data.},
keywords = {bootstrap, influence function, jackknife, statistics defined by functionals, time series, variance estimation},
year = {1989},
doi = {10.1214/aos/1176347265},
URL = {https://doi.org/10.1214/aos/1176347265}
}

@article{imbens2000,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2673642},
 abstract = {Estimation of average treatment effects in observational studies often requires adjustment for differences in pre-treatment variables. If the number of pre-treatment variables is large, standard covariance adjustment methods are often inadequate. Rosenbaum & Rubin (1983) propose an alternative method for adjusting for pre-treatment variables for the binary treatment case based on the so-called propensity score. Here an extension of the propensity score methodology is proposed that allows for estimation of average casual effects with multi-valued treatments.},
 author = {Guido W. Imbens},
 journal = {Biometrika},
 number = {3},
 pages = {706--710},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {The Role of the Propensity Score in Estimating Dose-Response Functions},
 urldate = {2022-09-26},
 volume = {87},
 year = {2000}
}

@article{zivich2022,
  doi = {10.48550/ARXIV.2207.05010},
  
  url = {https://arxiv.org/abs/2207.05010},
  
  author = {Zivich, Paul N and Cole, Stephen R and Westreich, Daniel},
  
  keywords = {Methodology (stat.ME), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Positivity: Identifiability and Estimability},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{quasipoisson,
    author = {WEDDERBURN, R. W. M.},
    title = "{Quasi-likelihood functions, generalized linear models, and the Gauss—Newton method}",
    journal = {Biometrika},
    volume = {61},
    number = {3},
    pages = {439-447},
    year = {1974},
    month = {12},
    abstract = "{To define a likelihood we have to specify the form of distribution of the observations, but to define a quasi-likelihood function we need only specify a relation between the mean and variance of the observations and the quasi-likelihood can then be used for estimation. For a one-parameter exponential family the log likelihood is the same as the quasi-likelihood and it follows that assuming a one-parameter exponential family is the weakest sort of distributional assumption that can be made. The Gauss-Newton method for calculating nonlinear least squares estimates generalizes easily to deal with maximum quasi-likelihood estimates, and a rearrangement of this produces a generalization of the method described by Nelder \\&amp; Wedderburn (1972).}",
    issn = {0006-3444},
    doi = {10.1093/biomet/61.3.439},
    url = {https://doi.org/10.1093/biomet/61.3.439},
    eprint = {https://academic.oup.com/biomet/article-pdf/61/3/439/690500/61-3-439.pdf},
}

@ARTICLE{dlnm2010R,
  title    = "Distributed lag linear and non-linear models in R: The package
              dlnm",
  author   = "Gasparrini, Antonio",
  abstract = "Distributed lag non-linear models (DLNMs) represent a modeling
              framework to flexibly describe associations showing potentially
              non-linear and delayed effects in time series data. This
              methodology rests on the definition of a crossbasis, a
              bi-dimensional functional space expressed by the combination of
              two sets of basis functions, which specify the relationships in
              the dimensions of predictor and lags, respectively. This
              framework is implemented in the R package dlnm, which provides
              functions to perform the broad range of models within the DLNM
              family and then to help interpret the results, with an emphasis
              on graphical representation. This paper offers an overview of the
              capabilities of the package, describing the conceptual and
              practical steps to specify and interpret DLNMs with an example of
              application to real data.",
  journal  = "J. Stat. Softw.",
  volume   =  43,
  number   =  8,
  pages    = "1--20",
  month    =  jul,
  year     =  2011,
  language = "en"
}

@article{dlnm2010,
author = {Gasparrini, A. and Armstrong, B. and Kenward, M. G.},
title = {Distributed lag non-linear models},
journal = {Statistics in Medicine},
volume = {29},
number = {21},
pages = {2224-2234},
keywords = {distributed lag, time series, smoothing, delayed effects},
doi = {https://doi.org/10.1002/sim.3940},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3940},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.3940},
abstract = {Abstract Environmental stressors often show effects that are delayed in time, requiring the use of statistical models that are flexible enough to describe the additional time dimension of the exposure–response relationship. Here we develop the family of distributed lag non-linear models (DLNM), a modelling framework that can simultaneously represent non-linear exposure–response dependencies and delayed effects. This methodology is based on the definition of a ‘cross-basis’, a bi-dimensional space of functions that describes simultaneously the shape of the relationship along both the space of the predictor and the lag dimension of its occurrence. In this way the approach provides a unified framework for a range of models that have previously been used in this setting, and new more flexible variants. This family of models is implemented in the package dlnm within the statistical environment R. To illustrate the methodology we use examples of DLNMs to represent the relationship between temperature and mortality, using data from the National Morbidity, Mortality, and Air Pollution Study (NMMAPS) for New York during the period 1987–2000. Copyright © 2010 John Wiley \& Sons, Ltd.},
year = {2010}
}

@article{wu2020sciadv,
author = {X. Wu  and D. Braun  and J. Schwartz  and M. A. Kioumourtzoglou  and F. Dominici },
title = {Evaluating the impact of long-term exposure to fine particulate matter on mortality among the elderly},
journal = {Science Advances},
volume = {6},
number = {29},
pages = {eaba5692},
year = {2020},
doi = {10.1126/sciadv.aba5692},
URL = {https://www.science.org/doi/abs/10.1126/sciadv.aba5692},
eprint = {https://www.science.org/doi/pdf/10.1126/sciadv.aba5692},
abstract = {Long-term exposure to fine particulate matter positively affects all-cause mortality among the elderly. Many studies link long-term fine particle (PM2.5) exposure to mortality, even at levels below current U.S. air quality standards (12 micrograms per cubic meter). These findings have been disputed with claims that the use of traditional statistical approaches does not guarantee causality. Leveraging 16 years of data—68.5 million Medicare enrollees—we provide strong evidence of the causal link between long-term PM2.5 exposure and mortality under a set of causal inference assumptions. Using five distinct approaches, we found that a decrease in PM2.5 (by 10 micrograms per cubic meter) leads to a statistically significant 6 to 7\% decrease in mortality risk. Based on these models, lowering the air quality standard to 10 micrograms per cubic meter would save 143,257 lives (95\% confidence interval, 115,581 to 170,645) in one decade. Our study provides the most comprehensive evidence to date of the link between long-term PM2.5 exposure and mortality, even at levels below current standards.}}

@article{bojinov2019,
author = {Iavor Bojinov and Neil Shephard},
title = {Time Series Experiments and Causal Estimands: Exact Randomization Tests and Trading},
journal = {Journal of the American Statistical Association},
volume = {114},
number = {528},
pages = {1665-1682},
year  = {2019},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2018.1527225},
URL = { 
        https://doi.org/10.1080/01621459.2018.1527225
},
eprint = { 
        https://doi.org/10.1080/01621459.2018.1527225
}
}

@article{gasparrini2016,
abstract = {This study assesses two alternative approaches for investigating linear and nonlinear lagged associations in environmental time series data, comparing through simulations simple methods based on moving average summaries with more flexible distributed lag linear and nonlinear models. Results indicate that the latter provide estimates with no or low bias and close-to-nominal confidence intervals, even for long-lagged associations and in the presence of strong seasonal trends. Moving average models represent a viable alternative only in the presence of relatively short lag periods, and when the lag interval is correctly specified. In contrast, the use of moving averages to roughly approximate long and complex lag patterns, or the specification of an interval different than the actual lag period, can result in substantial biases. More flexible approaches based on distributed lag linear or nonlinear models provide noteworthy advantages, in particular when complex lagged associations are assumed.},
author = {Gasparrini, Antonio},
issn = {1044-3983},
journal = {Epidemiology},
number = {6},
title = {{Modelling Lagged Associations in Environmental Time Series Data: A Simulation Study}},
url = {https://journals.lww.com/epidem/Fulltext/2016/11000/Modelling_Lagged_Associations_in_Environmental.9.aspx},
volume = {27},
year = {2016}
}


@article{dominici2019sci,
author = {Gretchen T. Goldman  and Francesca Dominici },
title = {Don't abandon evidence and process on air pollution policy},
journal = {Science},
volume = {363},
number = {6434},
pages = {1398-1400},
year = {2019},
doi = {10.1126/science.aaw9460},
URL = {https://www.science.org/doi/abs/10.1126/science.aaw9460},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aaw9460},
abstract = {Who decides how to establish causality? Air pollution kills—scientists have known this for many years. But how do they know? The global scientific community has developed and agreed upon a framework that draws on multiple lines of evidence across different scientific disciplines to assess the existence and strength of links between air pollution and health. In the United States, federal policies require use of this science-based framework to ensure that air pollution standards protect the public's health. But now this science-based policy process—and public health—are at risk. Recent developments at the U.S. Environmental Protection Agency (EPA) stand to quietly upend the time-tested and scientifically backed process the agency relies on to protect the public from ambient air pollution (1). One of these developments—changes in how the EPA handles causality between air pollutants and health effects—has received less attention but, if enacted, would alter the approach that the EPA has used for more than a decade to set health-based air pollutant standards. At the March meeting of the EPA's Clean Air Scientific Advisory Committee (CASAC) (2), these changes may begin to unfold. The agency now faces a dilemma. If the EPA leadership embraces the process proposed by the current CASAC chair, it will fundamentally change the EPA's process for scientific assessment. If the EPA leadership ignores the CASAC recommendations, then the agency would be declining to listen to (what should be) its top science advisers, thus eroding the foundational concept of peer review as central to ensuring the use of strong science in policy decisions}}

@article{rubin2008,
author = {Donald B. Rubin},
title = {{For objective causal inference, design trumps analysis}},
volume = {2},
journal = {The Annals of Applied Statistics},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {808 -- 840},
keywords = {Average causal effect, causal effects, complier average causal effect, instrumental variables, noncompliance, observational studies, propensity scores, randomized experiments, Rubin Causal Model},
year = {2008},
doi = {10.1214/08-AOAS187},
URL = {https://doi.org/10.1214/08-AOAS187}
}

@article{wu2020arxiv,
  doi = {10.48550/ARXIV.1812.06575},
  
  url = {https://arxiv.org/abs/1812.06575},
  
  author = {Wu, Xiao and Mealli, Fabrizia and Kioumourtzoglou, Marianthi-Anna and Dominici, Francesca and Braun, Danielle},
  
  keywords = {Methodology (stat.ME), Applications (stat.AP), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Matching on Generalized Propensity Scores with Continuous Exposures},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{sipw2010,
title = {Use of Stabilized Inverse Propensity Scores as Weights to Directly Estimate Relative Risk and Its Confidence Intervals},
journal = {Value in Health},
volume = {13},
number = {2},
pages = {273-277},
year = {2010},
issn = {1098-3015},
doi = {https://doi.org/10.1111/j.1524-4733.2009.00671.x},
url = {https://www.sciencedirect.com/science/article/pii/S1098301510603725},
author = {Stanley Xu and Colleen Ross and Marsha A. Raebel and Susan Shetterly and Christopher Blanchette and David Smith},
keywords = {confidence intervals, incidence rate ratio, inverse probability of treatment weighting, stabilized weights, type I error rates},
abstract = {ABSTRACT
Objectives
Inverse probability of treatment weighting (IPTW) has been used in observational studies to reduce selection bias. For estimates of the main effects to be obtained, a pseudo data set is created by weighting each subject by IPTW and analyzed with conventional regression models. Currently, variance estimation requires additional work depending on type of outcomes. Our goal is to demonstrate a statistical approach to directly obtain appropriate estimates of variance of the main effects in regression models.
Methods
We carried out theoretical and simulation studies to show that the variance of the main effects estimated directly from regressions using IPTW is underestimated and that the type I error rate is higher because of the inflated sample size in the pseudo data. The robust variance estimator using IPTW often slightly overestimates the variance of the main effects. We propose to use the stabilized weights to directly estimate both the main effect and its variance from conventional regression models.
Results
We applied the approach to a study examining the effectiveness of serum potassium monitoring in reducing hyperkalemia-associated adverse events among 27,355 diabetic patients newly prescribed with a renin-angiotensin-aldosterone system inhibitor. The incidence rate ratio (with monitoring vs. without monitoring) and confidence intervals were 0.46 (0.34, 0.61) using the stabilized weights compared with 0.46 (0.38, 0.55) using typical IPTW.
Conclusions
Our theoretical, simulation results and real data example demonstrate that the use of the stabilized weights in the pseudo data preserves the sample size of the original data, produces appropriate estimation of the variance of main effect, and maintains an appropriate type I error rate.}
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{gasparrini2015,
  title     = "Mortality risk attributable to high and low ambient temperature:
               a multicountry observational study",
  author    = "Gasparrini, Antonio and Guo, Yuming and Hashizume, Masahiro and
               Lavigne, Eric and Zanobetti, Antonella and Schwartz, Joel and
               Tobias, Aurelio and Tong, Shilu and Rockl{\"o}v, Joacim and
               Forsberg, Bertil and Leone, Michela and De Sario, Manuela and
               Bell, Michelle L and Guo, Yue-Liang Leon and Wu, Chang-Fu and
               Kan, Haidong and Yi, Seung-Muk and de Sousa Zanotti Stagliorio
               Coelho, Micheline and Saldiva, Paulo Hilario Nascimento and
               Honda, Yasushi and Kim, Ho and Armstrong, Ben",
  abstract  = "BACKGROUND: Although studies have provided estimates of
               premature deaths attributable to either heat or cold in selected
               countries, none has so far offered a systematic assessment
               across the whole temperature range in populations exposed to
               different climates. We aimed to quantify the total mortality
               burden attributable to non-optimum ambient temperature, and the
               relative contributions from heat and cold and from moderate and
               extreme temperatures. METHODS: We collected data for 384
               locations in Australia, Brazil, Canada, China, Italy, Japan,
               South Korea, Spain, Sweden, Taiwan, Thailand, UK, and USA. We
               fitted a standard time-series Poisson model for each location,
               controlling for trends and day of the week. We estimated
               temperature-mortality associations with a distributed lag
               non-linear model with 21 days of lag, and then pooled them in a
               multivariate metaregression that included country indicators and
               temperature average and range. We calculated attributable deaths
               for heat and cold, defined as temperatures above and below the
               optimum temperature, which corresponded to the point of minimum
               mortality, and for moderate and extreme temperatures, defined
               using cutoffs at the 2·5th and 97·5th temperature percentiles.
               FINDINGS: We analysed 74,225,200 deaths in various periods
               between 1985 and 2012. In total, 7·71\% (95\% empirical CI
               7·43-7·91) of mortality was attributable to non-optimum
               temperature in the selected countries within the study period,
               with substantial differences between countries, ranging from
               3·37\% (3·06 to 3·63) in Thailand to 11·00\% (9·29 to 12·47) in
               China. The temperature percentile of minimum mortality varied
               from roughly the 60th percentile in tropical areas to about the
               80-90th percentile in temperate regions. More
               temperature-attributable deaths were caused by cold (7·29\%,
               7·02-7·49) than by heat (0·42\%, 0·39-0·44). Extreme cold and
               hot temperatures were responsible for 0·86\% (0·84-0·87) of
               total mortality. INTERPRETATION: Most of the temperature-related
               mortality burden was attributable to the contribution of cold.
               The effect of days of extreme temperature was substantially less
               than that attributable to milder but non-optimum weather. This
               evidence has important implications for the planning of
               public-health interventions to minimise the health consequences
               of adverse temperatures, and for predictions of future effect in
               climate-change scenarios. FUNDING: UK Medical Research Council.",
  journal   = "Lancet",
  publisher = "Elsevier BV",
  volume    =  386,
  number    =  9991,
  pages     = "369--375",
  month     =  jul,
  year      =  2015,
  copyright = "http://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}


@article{gasparrini2017,
title = {Projections of temperature-related excess mortality under climate change scenarios},
journal = {The Lancet Planetary Health},
volume = {1},
number = {9},
pages = {e360-e367},
year = {2017},
issn = {2542-5196},
doi = {https://doi.org/10.1016/S2542-5196(17)30156-0},
url = {https://www.sciencedirect.com/science/article/pii/S2542519617301560},
author = {Antonio Gasparrini and Yuming Guo and Francesco Sera and Ana Maria Vicedo-Cabrera and Veronika Huber and Shilu Tong and Micheline {de Sousa Zanotti Stagliorio Coelho} and Paulo Hilario {Nascimento Saldiva} and Eric Lavigne and Patricia {Matus Correa} and Nicolas {Valdes Ortega} and Haidong Kan and Samuel Osorio and Jan Kyselý and Aleš Urban and Jouni J K Jaakkola and Niilo R I Ryti and Mathilde Pascal and Patrick G Goodman and Ariana Zeka and Paola Michelozzi and Matteo Scortichini and Masahiro Hashizume and Yasushi Honda and Magali Hurtado-Diaz and Julio {Cesar Cruz} and Xerxes Seposo and Ho Kim and Aurelio Tobias and Carmen Iñiguez and Bertil Forsberg and Daniel Oudin Åström and Martina S Ragettli and Yue Leon Guo and Chang-fu Wu and Antonella Zanobetti and Joel Schwartz and Michelle L Bell and Tran Ngoc Dang and Dung Do Van and Clare Heaviside and Sotiris Vardoulakis and Shakoor Hajat and Andy Haines and Ben Armstrong},
abstract = {Summary
Background
Climate change can directly affect human health by varying exposure to non-optimal outdoor temperature. However, evidence on this direct impact at a global scale is limited, mainly due to issues in modelling and projecting complex and highly heterogeneous epidemiological relationships across different populations and climates.
Methods
We collected observed daily time series of mean temperature and mortality counts for all causes or non-external causes only, in periods ranging from Jan 1, 1984, to Dec 31, 2015, from various locations across the globe through the Multi-Country Multi-City Collaborative Research Network. We estimated temperature–mortality relationships through a two-stage time series design. We generated current and future daily mean temperature series under four scenarios of climate change, determined by varying trajectories of greenhouse gas emissions, using five general circulation models. We projected excess mortality for cold and heat and their net change in 1990–2099 under each scenario of climate change, assuming no adaptation or population changes.
Findings
Our dataset comprised 451 locations in 23 countries across nine regions of the world, including 85 879 895 deaths. Results indicate, on average, a net increase in temperature-related excess mortality under high-emission scenarios, although with important geographical differences. In temperate areas such as northern Europe, east Asia, and Australia, the less intense warming and large decrease in cold-related excess would induce a null or marginally negative net effect, with the net change in 2090–99 compared with 2010–19 ranging from −1·2% (empirical 95% CI −3·6 to 1·4) in Australia to −0·1% (−2·1 to 1·6) in east Asia under the highest emission scenario, although the decreasing trends would reverse during the course of the century. Conversely, warmer regions, such as the central and southern parts of America or Europe, and especially southeast Asia, would experience a sharp surge in heat-related impacts and extremely large net increases, with the net change at the end of the century ranging from 3·0% (−3·0 to 9·3) in Central America to 12·7% (−4·7 to 28·1) in southeast Asia under the highest emission scenario. Most of the health effects directly due to temperature increase could be avoided under scenarios involving mitigation strategies to limit emissions and further warming of the planet.
Interpretation
This study shows the negative health impacts of climate change that, under high-emission scenarios, would disproportionately affect warmer and poorer regions of the world. Comparison with lower emission scenarios emphasises the importance of mitigation policies for limiting global warming and reducing the associated health risks.
Funding
UK Medical Research Council.}
}

@article{yoonhee2019,
author = {Yoonhee Kim  and Ho Kim  and Antonio Gasparrini  and Ben Armstrong  and Yasushi Honda  and Yeonseung Chung  and Chris Fook Sheng Ng  and Aurelio Tobias  and Carmen Íñiguez  and Eric Lavigne  and Francesco Sera  and Ana M. Vicedo-Cabrera  and Martina S. Ragettli  and Noah Scovronick  and Fiorella Acquaotta  and Bing-Yu Chen  and Yue-Liang Leon Guo  and Xerxes Seposo  and Tran Ngoc Dang  and Micheline de Sousa Zanotti Stagliorio Coelho  and Paulo Hilario Nascimento Saldiva  and Anna Kosheleva  and Antonella Zanobetti  and Joel Schwartz  and Michelle L. Bell  and Masahiro Hashizume },
title = {Suicide and Ambient Temperature: A Multi-Country Multi-City Study},
journal = {Environmental Health Perspectives},
volume = {127},
number = {11},
pages = {117007},
year = {2019},
doi = {10.1289/EHP4898},

URL = {https://ehp.niehs.nih.gov/doi/abs/10.1289/EHP4898},
eprint = {https://ehp.niehs.nih.gov/doi/pdf/10.1289/EHP4898}

}

@book{whatif2020,
author = {Hernán MA and Robins JM},
title = {Causal Inference: What If},
year = {2020},
publisher = {Chapman \& Hall/CRC}
}

@article{rubin2007sim,
author = {Rubin, Donald B.},
title = {The design versus the analysis of observational studies for causal effects: parallels with the design of randomized trials},
journal = {Statistics in Medicine},
volume = {26},
number = {1},
pages = {20-36},
keywords = {assignment mechanism, causal inference, objective design, propensity scores, Rubin causal model, tobacco litigation},
doi = {https://doi.org/10.1002/sim.2739},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.2739},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.2739},
abstract = {Abstract For estimating causal effects of treatments, randomized experiments are generally considered the gold standard. Nevertheless, they are often infeasible to conduct for a variety of reasons, such as ethical concerns, excessive expense, or timeliness. Consequently, much of our knowledge of causal effects must come from non-randomized observational studies. This article will advocate the position that observational studies can and should be designed to approximate randomized experiments as closely as possible. In particular, observational studies should be designed using only background information to create subgroups of similar treated and control units, where ‘similar’ here refers to their distributions of background variables. Of great importance, this activity should be conducted without any access to any outcome data, thereby assuring the objectivity of the design. In many situations, this objective creation of subgroups of similar treated and control units, which are balanced with respect to covariates, can be accomplished using propensity score methods. The theoretical perspective underlying this position will be presented followed by a particular application in the context of the US tobacco litigation. This application uses propensity score methods to create subgroups of treated units (male current smokers) and control units (male never smokers) who are at least as similar with respect to their distributions of observed background characteristics as if they had been randomized. The collection of these subgroups then ‘approximate’ a randomized block experiment with respect to the observed covariates. Copyright © 2006 John Wiley \& Sons, Ltd.},
year = {2007}
}

@article{temperaturemorbidity,
author = {Xiaofang Ye  and Rodney Wolff  and Weiwei Yu  and Pavla Vaneckova  and Xiaochuan Pan  and Shilu Tong },
title = {Ambient Temperature and Morbidity: A Review of Epidemiological Evidence},
journal = {Environmental Health Perspectives},
volume = {120},
number = {1},
pages = {19-28},
year = {2012},
doi = {10.1289/ehp.1003198},

URL = {https://ehp.niehs.nih.gov/doi/abs/10.1289/ehp.1003198},
eprint = {https://ehp.niehs.nih.gov/doi/pdf/10.1289/ehp.1003198}
,
    abstract = { Objective: In this paper, we review the epidemiological evidence on the relationship between ambient temperature and morbidity. We assessed the methodological issues in previous studies and proposed future research directions. Data sources and data extraction: We searched the PubMed database for epidemiological studies on ambient temperature and morbidity of noncommunicable diseases published in refereed English journals before 30 June 2010. Forty relevant studies were identified. Of these, 24 examined the relationship between ambient temperature and morbidity, 15 investigated the short-term effects of heat wave on morbidity, and 1 assessed both temperature and heat wave effects. Data synthesis: Descriptive and time-series studies were the two main research designs used to investigate the temperature–morbidity relationship. Measurements of temperature exposure and health outcomes used in these studies differed widely. The majority of studies reported a significant relationship between ambient temperature and total or cause-specific morbidities. However, there were some inconsistencies in the direction and magnitude of nonlinear lag effects. The lag effect of hot temperature on morbidity was shorter (several days) compared with that of cold temperature (up to a few weeks). The temperature–morbidity relationship may be confounded or modified by sociodemographic factors and air pollution. Conclusions: There is a significant short-term effect of ambient temperature on total and cause-specific morbidities. However, further research is needed to determine an appropriate temperature measure, consider a diverse range of morbidities, and to use consistent methodology to make different studies more comparable. }
}


@article{tobias2017,
  title    = "Brief report: Investigating uncertainty in the minimum mortality
              temperature: Methods and application to 52 Spanish cities",
  author   = "Tob{\'\i}as, Aurelio and Armstrong, Ben and Gasparrini, Antonio",
  abstract = "BACKGROUND: The minimum mortality temperature from J- or U-shaped
              curves varies across cities with different climates. This
              variation conveys information on adaptation, but ability to
              characterize is limited by the absence of a method to describe
              uncertainty in estimated minimum mortality temperatures. METHODS:
              We propose an approximate parametric bootstrap estimator of
              confidence interval (CI) and standard error (SE) for the minimum
              mortality temperature from a temperature-mortality shape
              estimated by splines. RESULTS: The coverage of the estimated CIs
              was close to nominal value (95\%) in the datasets simulated,
              although SEs were slightly high. Applying the method to 52
              Spanish provincial capital cities showed larger minimum mortality
              temperatures in hotter cities, rising almost exactly at the same
              rate as annual mean temperature. CONCLUSIONS: The method proposed
              for computing CIs and SEs for minimums from spline curves allows
              comparing minimum mortality temperatures in different cities and
              investigating their associations with climate properly, allowing
              for estimation uncertainty.",
  journal  = "Epidemiology",
  volume   =  28,
  number   =  1,
  pages    = "72--76",
  month    =  jan,
  year     =  2017,
  language = "en"
}

@book{banerjee2015,
   title =     {Hierarchical Modeling and Analysis for Spatial Data, Second Edition},
   author =    {Banerjee, Sudipto and Carlin, Bradley P. and Gelfand, Alan E},
   publisher = {CRC Press},
   isbn =      {978-1-4398-1918-0,1439819181},
   year =      {2015},
   series =    {Chapman \& Hall/CRC Monographs on Statistics \& Applied Probability},
   edition =   {2ed.},
   volume =    {},
   url =       {http://gen.lib.rus.ec/book/index.php?md5=ee60a1077624ddac0ebb2042fdf2098e}
}